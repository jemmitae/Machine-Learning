{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "## SVM & Regularization\n",
    "\n",
    "\n",
    "For this homework we consider a set of observations on a number of red and white wine varieties involving their chemical properties and ranking by tasters. Wine industry shows a recent growth spurt as social drinking is on the rise. The price of wine depends on a rather abstract concept of wine appreciation by wine tasters, opinion among whom may have a high degree of variability. Pricing of wine depends on such a volatile factor to some extent. Another key factor in wine certification and quality assessment is physicochemical tests which are laboratory-based and takes into account factors like acidity, pH level, presence of sugar and other chemical properties. For the wine market, it would be of interest if human quality of tasting can be related to the chemical properties of wine so that certification and quality assessment and assurance process is more controlled.\n",
    "\n",
    "Two datasets are available of which one dataset is on red wine and have 1599 different varieties and the other is on white wine and have 4898 varieties. All wines are produced in a particular area of Portugal. Data are collected on 12 different properties of the wines one of which is Quality, based on sensory data, and the rest are on chemical properties of the wines including density, acidity, alcohol content etc. All chemical properties of wines are continuous variables. Quality is an ordinal variable with possible ranking from 1 (worst) to 10 (best). Each variety of wine is tasted by three independent tasters and the final rank assigned is the median rank given by the tasters.\n",
    "\n",
    "A predictive model developed on this data is expected to provide guidance to vineyards regarding quality and price expected on their produce without heavy reliance on volatility of wine tasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_red.csv')\n",
    "data_w = pd.read_csv('https://github.com/albahnsen/PracticalMachineLearningClass/raw/master/datasets/Wine_data_white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>17.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.99270</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.054</td>\n",
       "      <td>26.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.99489</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.33</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.048</td>\n",
       "      <td>45.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.99472</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.031</td>\n",
       "      <td>41.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.99320</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>11.8</td>\n",
       "      <td>7</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "378             5.7              0.32         0.50             2.6      0.049   \n",
       "3036            6.8              0.29         0.34             3.5      0.054   \n",
       "3330            6.7              0.23         0.33             8.1      0.048   \n",
       "1868            7.4              0.21         0.27             7.3      0.031   \n",
       "20              6.2              0.66         0.48             1.2      0.029   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "378                  17.0                 155.0  0.99270  3.22       0.64   \n",
       "3036                 26.0                 189.0  0.99489  3.42       0.58   \n",
       "3330                 45.0                 176.0  0.99472  3.11       0.52   \n",
       "1868                 41.0                 144.0  0.99320  3.15       0.38   \n",
       "20                   29.0                  75.0  0.98920  3.33       0.39   \n",
       "\n",
       "      alcohol  quality   type  \n",
       "378      10.0        6  white  \n",
       "3036     10.4        5  white  \n",
       "3330     10.1        6  white  \n",
       "1868     11.8        7  white  \n",
       "20       12.8        8  white  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_w.assign(type = 'white')\n",
    "data = data.append(data_r.assign(type = 'red'), ignore_index=True)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.1\n",
    "\n",
    "Show the frecuency table of the quality by type of wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>163</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>681</td>\n",
       "      <td>1457</td>\n",
       "      <td>2138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>638</td>\n",
       "      <td>2198</td>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>199</td>\n",
       "      <td>880</td>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>175</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1599</td>\n",
       "      <td>4898</td>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type      red  white   All\n",
       "quality                   \n",
       "3          10     20    30\n",
       "4          53    163   216\n",
       "5         681   1457  2138\n",
       "6         638   2198  2836\n",
       "7         199    880  1079\n",
       "8          18    175   193\n",
       "9           0      5     5\n",
       "All      1599   4898  6497"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=data['quality'], columns=data['type'], margins=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.2\n",
    "\n",
    "* Standarized the features (not the quality)  ---Normalizar\n",
    "* Create a binary target for each type of wine\n",
    "* Create two Linear SVM's for the white and red wines, repectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0               7.0             0.270         0.36           20.70      0.045   \n",
      "1               6.3             0.300         0.34            1.60      0.049   \n",
      "2               8.1             0.280         0.40            6.90      0.050   \n",
      "3               7.2             0.230         0.32            8.50      0.058   \n",
      "4               7.2             0.230         0.32            8.50      0.058   \n",
      "5               8.1             0.280         0.40            6.90      0.050   \n",
      "6               6.2             0.320         0.16            7.00      0.045   \n",
      "7               7.0             0.270         0.36           20.70      0.045   \n",
      "8               6.3             0.300         0.34            1.60      0.049   \n",
      "9               8.1             0.220         0.43            1.50      0.044   \n",
      "10              8.1             0.270         0.41            1.45      0.033   \n",
      "11              8.6             0.230         0.40            4.20      0.035   \n",
      "12              7.9             0.180         0.37            1.20      0.040   \n",
      "13              6.6             0.160         0.40            1.50      0.044   \n",
      "14              8.3             0.420         0.62           19.25      0.040   \n",
      "15              6.6             0.170         0.38            1.50      0.032   \n",
      "16              6.3             0.480         0.04            1.10      0.046   \n",
      "17              6.2             0.660         0.48            1.20      0.029   \n",
      "18              7.4             0.340         0.42            1.10      0.033   \n",
      "19              6.5             0.310         0.14            7.50      0.044   \n",
      "20              6.2             0.660         0.48            1.20      0.029   \n",
      "21              6.4             0.310         0.38            2.90      0.038   \n",
      "22              6.8             0.260         0.42            1.70      0.049   \n",
      "23              7.6             0.670         0.14            1.50      0.074   \n",
      "24              6.6             0.270         0.41            1.30      0.052   \n",
      "25              7.0             0.250         0.32            9.00      0.046   \n",
      "26              6.9             0.240         0.35            1.00      0.052   \n",
      "27              7.0             0.280         0.39            8.70      0.051   \n",
      "28              7.4             0.270         0.48            1.10      0.047   \n",
      "29              7.2             0.320         0.36            2.00      0.033   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "6467            6.2             0.510         0.14            1.90      0.056   \n",
      "6468            6.4             0.360         0.53            2.20      0.230   \n",
      "6469            6.4             0.380         0.14            2.20      0.038   \n",
      "6470            7.3             0.690         0.32            2.20      0.069   \n",
      "6471            6.0             0.580         0.20            2.40      0.075   \n",
      "6472            5.6             0.310         0.78           13.90      0.074   \n",
      "6473            7.5             0.520         0.40            2.20      0.060   \n",
      "6474            8.0             0.300         0.63            1.60      0.081   \n",
      "6475            6.2             0.700         0.15            5.10      0.076   \n",
      "6476            6.8             0.670         0.15            1.80      0.118   \n",
      "6477            6.2             0.560         0.09            1.70      0.053   \n",
      "6478            7.4             0.350         0.33            2.40      0.068   \n",
      "6479            6.2             0.560         0.09            1.70      0.053   \n",
      "6480            6.1             0.715         0.10            2.60      0.053   \n",
      "6481            6.2             0.460         0.29            2.10      0.074   \n",
      "6482            6.7             0.320         0.44            2.40      0.061   \n",
      "6483            7.2             0.390         0.44            2.60      0.066   \n",
      "6484            7.5             0.310         0.41            2.40      0.065   \n",
      "6485            5.8             0.610         0.11            1.80      0.066   \n",
      "6486            7.2             0.660         0.33            2.50      0.068   \n",
      "6487            6.6             0.725         0.20            7.80      0.073   \n",
      "6488            6.3             0.550         0.15            1.80      0.077   \n",
      "6489            5.4             0.740         0.09            1.70      0.089   \n",
      "6490            6.3             0.510         0.13            2.30      0.076   \n",
      "6491            6.8             0.620         0.08            1.90      0.068   \n",
      "6492            6.2             0.600         0.08            2.00      0.090   \n",
      "6493            5.9             0.550         0.10            2.20      0.062   \n",
      "6494            6.3             0.510         0.13            2.30      0.076   \n",
      "6495            5.9             0.645         0.12            2.00      0.075   \n",
      "6496            6.0             0.310         0.47            3.60      0.067   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
      "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
      "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
      "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
      "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
      "5                    30.0                  97.0  0.99510  3.26       0.44   \n",
      "6                    30.0                 136.0  0.99490  3.18       0.47   \n",
      "7                    45.0                 170.0  1.00100  3.00       0.45   \n",
      "8                    14.0                 132.0  0.99400  3.30       0.49   \n",
      "9                    28.0                 129.0  0.99380  3.22       0.45   \n",
      "10                   11.0                  63.0  0.99080  2.99       0.56   \n",
      "11                   17.0                 109.0  0.99470  3.14       0.53   \n",
      "12                   16.0                  75.0  0.99200  3.18       0.63   \n",
      "13                   48.0                 143.0  0.99120  3.54       0.52   \n",
      "14                   41.0                 172.0  1.00020  2.98       0.67   \n",
      "15                   28.0                 112.0  0.99140  3.25       0.55   \n",
      "16                   30.0                  99.0  0.99280  3.24       0.36   \n",
      "17                   29.0                  75.0  0.98920  3.33       0.39   \n",
      "18                   17.0                 171.0  0.99170  3.12       0.53   \n",
      "19                   34.0                 133.0  0.99550  3.22       0.50   \n",
      "20                   29.0                  75.0  0.98920  3.33       0.39   \n",
      "21                   19.0                 102.0  0.99120  3.17       0.35   \n",
      "22                   41.0                 122.0  0.99300  3.47       0.48   \n",
      "23                   25.0                 168.0  0.99370  3.05       0.51   \n",
      "24                   16.0                 142.0  0.99510  3.42       0.47   \n",
      "25                   56.0                 245.0  0.99550  3.25       0.50   \n",
      "26                   35.0                 146.0  0.99300  3.45       0.44   \n",
      "27                   32.0                 141.0  0.99610  3.38       0.53   \n",
      "28                   17.0                 132.0  0.99140  3.19       0.49   \n",
      "29                   37.0                 114.0  0.99060  3.10       0.71   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "6467                 15.0                  34.0  0.99396  3.48       0.57   \n",
      "6468                 19.0                  35.0  0.99340  3.37       0.93   \n",
      "6469                 15.0                  25.0  0.99514  3.44       0.65   \n",
      "6470                 35.0                 104.0  0.99632  3.33       0.51   \n",
      "6471                 15.0                  50.0  0.99467  3.58       0.67   \n",
      "6472                 23.0                  92.0  0.99677  3.39       0.48   \n",
      "6473                 12.0                  20.0  0.99474  3.26       0.64   \n",
      "6474                 16.0                  29.0  0.99588  3.30       0.78   \n",
      "6475                 13.0                  27.0  0.99622  3.54       0.60   \n",
      "6476                 13.0                  20.0  0.99540  3.42       0.67   \n",
      "6477                 24.0                  32.0  0.99402  3.54       0.60   \n",
      "6478                  9.0                  26.0  0.99470  3.36       0.60   \n",
      "6479                 24.0                  32.0  0.99402  3.54       0.60   \n",
      "6480                 13.0                  27.0  0.99362  3.57       0.50   \n",
      "6481                 32.0                  98.0  0.99578  3.33       0.62   \n",
      "6482                 24.0                  34.0  0.99484  3.29       0.80   \n",
      "6483                 22.0                  48.0  0.99494  3.30       0.84   \n",
      "6484                 34.0                  60.0  0.99492  3.34       0.85   \n",
      "6485                 18.0                  28.0  0.99483  3.55       0.66   \n",
      "6486                 34.0                 102.0  0.99414  3.27       0.78   \n",
      "6487                 29.0                  79.0  0.99770  3.29       0.54   \n",
      "6488                 26.0                  35.0  0.99314  3.32       0.82   \n",
      "6489                 16.0                  26.0  0.99402  3.67       0.56   \n",
      "6490                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "6491                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "6492                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "6493                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "6494                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "6495                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "6496                 18.0                  42.0  0.99549  3.39       0.66   \n",
      "\n",
      "      alcohol  \n",
      "0         8.8  \n",
      "1         9.5  \n",
      "2        10.1  \n",
      "3         9.9  \n",
      "4         9.9  \n",
      "5        10.1  \n",
      "6         9.6  \n",
      "7         8.8  \n",
      "8         9.5  \n",
      "9        11.0  \n",
      "10       12.0  \n",
      "11        9.7  \n",
      "12       10.8  \n",
      "13       12.4  \n",
      "14        9.7  \n",
      "15       11.4  \n",
      "16        9.6  \n",
      "17       12.8  \n",
      "18       11.3  \n",
      "19        9.5  \n",
      "20       12.8  \n",
      "21       11.0  \n",
      "22       10.5  \n",
      "23        9.3  \n",
      "24       10.0  \n",
      "25       10.4  \n",
      "26       10.0  \n",
      "27       10.5  \n",
      "28       11.6  \n",
      "29       12.3  \n",
      "...       ...  \n",
      "6467     11.5  \n",
      "6468     12.4  \n",
      "6469     11.1  \n",
      "6470      9.5  \n",
      "6471     12.5  \n",
      "6472     10.5  \n",
      "6473     11.8  \n",
      "6474     10.8  \n",
      "6475     11.9  \n",
      "6476     11.3  \n",
      "6477     11.3  \n",
      "6478     11.9  \n",
      "6479     11.3  \n",
      "6480     11.9  \n",
      "6481      9.8  \n",
      "6482     11.6  \n",
      "6483     11.5  \n",
      "6484     11.4  \n",
      "6485     10.9  \n",
      "6486     12.8  \n",
      "6487      9.2  \n",
      "6488     11.6  \n",
      "6489     11.6  \n",
      "6490     11.0  \n",
      "6491      9.5  \n",
      "6492     10.5  \n",
      "6493     11.2  \n",
      "6494     11.0  \n",
      "6495     10.2  \n",
      "6496     11.0  \n",
      "\n",
      "[6497 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "data_not = data.drop('type',1)\n",
    "data_not=data_not.drop('quality',1)\n",
    "print(data_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0          0.264463          0.126667     0.216867        0.308282   0.059801   \n",
      "1          0.206612          0.146667     0.204819        0.015337   0.066445   \n",
      "2          0.355372          0.133333     0.240964        0.096626   0.068106   \n",
      "3          0.280992          0.100000     0.192771        0.121166   0.081395   \n",
      "4          0.280992          0.100000     0.192771        0.121166   0.081395   \n",
      "5          0.355372          0.133333     0.240964        0.096626   0.068106   \n",
      "6          0.198347          0.160000     0.096386        0.098160   0.059801   \n",
      "7          0.264463          0.126667     0.216867        0.308282   0.059801   \n",
      "8          0.206612          0.146667     0.204819        0.015337   0.066445   \n",
      "9          0.355372          0.093333     0.259036        0.013804   0.058140   \n",
      "10         0.355372          0.126667     0.246988        0.013037   0.039867   \n",
      "11         0.396694          0.100000     0.240964        0.055215   0.043189   \n",
      "12         0.338843          0.066667     0.222892        0.009202   0.051495   \n",
      "13         0.231405          0.053333     0.240964        0.013804   0.058140   \n",
      "14         0.371901          0.226667     0.373494        0.286043   0.051495   \n",
      "15         0.231405          0.060000     0.228916        0.013804   0.038206   \n",
      "16         0.206612          0.266667     0.024096        0.007669   0.061462   \n",
      "17         0.198347          0.386667     0.289157        0.009202   0.033223   \n",
      "18         0.297521          0.173333     0.253012        0.007669   0.039867   \n",
      "19         0.223140          0.153333     0.084337        0.105828   0.058140   \n",
      "20         0.198347          0.386667     0.289157        0.009202   0.033223   \n",
      "21         0.214876          0.153333     0.228916        0.035276   0.048173   \n",
      "22         0.247934          0.120000     0.253012        0.016871   0.066445   \n",
      "23         0.314050          0.393333     0.084337        0.013804   0.107973   \n",
      "24         0.231405          0.126667     0.246988        0.010736   0.071429   \n",
      "25         0.264463          0.113333     0.192771        0.128834   0.061462   \n",
      "26         0.256198          0.106667     0.210843        0.006135   0.071429   \n",
      "27         0.264463          0.133333     0.234940        0.124233   0.069767   \n",
      "28         0.297521          0.126667     0.289157        0.007669   0.063123   \n",
      "29         0.280992          0.160000     0.216867        0.021472   0.039867   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "6467       0.198347          0.286667     0.084337        0.019939   0.078073   \n",
      "6468       0.214876          0.186667     0.319277        0.024540   0.367110   \n",
      "6469       0.214876          0.200000     0.084337        0.024540   0.048173   \n",
      "6470       0.289256          0.406667     0.192771        0.024540   0.099668   \n",
      "6471       0.181818          0.333333     0.120482        0.027607   0.109635   \n",
      "6472       0.148760          0.153333     0.469880        0.203988   0.107973   \n",
      "6473       0.305785          0.293333     0.240964        0.024540   0.084718   \n",
      "6474       0.347107          0.146667     0.379518        0.015337   0.119601   \n",
      "6475       0.198347          0.413333     0.090361        0.069018   0.111296   \n",
      "6476       0.247934          0.393333     0.090361        0.018405   0.181063   \n",
      "6477       0.198347          0.320000     0.054217        0.016871   0.073090   \n",
      "6478       0.297521          0.180000     0.198795        0.027607   0.098007   \n",
      "6479       0.198347          0.320000     0.054217        0.016871   0.073090   \n",
      "6480       0.190083          0.423333     0.060241        0.030675   0.073090   \n",
      "6481       0.198347          0.253333     0.174699        0.023006   0.107973   \n",
      "6482       0.239669          0.160000     0.265060        0.027607   0.086379   \n",
      "6483       0.280992          0.206667     0.265060        0.030675   0.094684   \n",
      "6484       0.305785          0.153333     0.246988        0.027607   0.093023   \n",
      "6485       0.165289          0.353333     0.066265        0.018405   0.094684   \n",
      "6486       0.280992          0.386667     0.198795        0.029141   0.098007   \n",
      "6487       0.231405          0.430000     0.120482        0.110429   0.106312   \n",
      "6488       0.206612          0.313333     0.090361        0.018405   0.112957   \n",
      "6489       0.132231          0.440000     0.054217        0.016871   0.132890   \n",
      "6490       0.206612          0.286667     0.078313        0.026074   0.111296   \n",
      "6491       0.247934          0.360000     0.048193        0.019939   0.098007   \n",
      "6492       0.198347          0.346667     0.048193        0.021472   0.134551   \n",
      "6493       0.173554          0.313333     0.060241        0.024540   0.088040   \n",
      "6494       0.206612          0.286667     0.078313        0.026074   0.111296   \n",
      "6495       0.173554          0.376667     0.072289        0.021472   0.109635   \n",
      "6496       0.181818          0.153333     0.283133        0.046012   0.096346   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
      "0                0.152778              0.377880  0.267785  0.217054   \n",
      "1                0.045139              0.290323  0.132832  0.449612   \n",
      "2                0.100694              0.209677  0.154039  0.418605   \n",
      "3                0.159722              0.414747  0.163678  0.364341   \n",
      "4                0.159722              0.414747  0.163678  0.364341   \n",
      "5                0.100694              0.209677  0.154039  0.418605   \n",
      "6                0.100694              0.299539  0.150183  0.356589   \n",
      "7                0.152778              0.377880  0.267785  0.217054   \n",
      "8                0.045139              0.290323  0.132832  0.449612   \n",
      "9                0.093750              0.283410  0.128976  0.387597   \n",
      "10               0.034722              0.131336  0.071139  0.209302   \n",
      "11               0.055556              0.237327  0.146327  0.325581   \n",
      "12               0.052083              0.158986  0.094274  0.356589   \n",
      "13               0.163194              0.315668  0.078851  0.635659   \n",
      "14               0.138889              0.382488  0.252362  0.201550   \n",
      "15               0.093750              0.244240  0.082707  0.410853   \n",
      "16               0.100694              0.214286  0.109697  0.403101   \n",
      "17               0.097222              0.158986  0.040293  0.472868   \n",
      "18               0.055556              0.380184  0.088490  0.310078   \n",
      "19               0.114583              0.292627  0.161751  0.387597   \n",
      "20               0.097222              0.158986  0.040293  0.472868   \n",
      "21               0.062500              0.221198  0.078851  0.348837   \n",
      "22               0.138889              0.267281  0.113553  0.581395   \n",
      "23               0.083333              0.373272  0.127048  0.255814   \n",
      "24               0.052083              0.313364  0.154039  0.542636   \n",
      "25               0.190972              0.550691  0.161751  0.410853   \n",
      "26               0.118056              0.322581  0.113553  0.565891   \n",
      "27               0.107639              0.311060  0.173318  0.511628   \n",
      "28               0.055556              0.290323  0.082707  0.364341   \n",
      "29               0.125000              0.248848  0.067284  0.294574   \n",
      "...                   ...                   ...       ...       ...   \n",
      "6467             0.048611              0.064516  0.132061  0.589147   \n",
      "6468             0.062500              0.066820  0.121265  0.503876   \n",
      "6469             0.048611              0.043779  0.154810  0.558140   \n",
      "6470             0.118056              0.225806  0.177559  0.472868   \n",
      "6471             0.048611              0.101382  0.145749  0.666667   \n",
      "6472             0.076389              0.198157  0.186235  0.519380   \n",
      "6473             0.038194              0.032258  0.147099  0.418605   \n",
      "6474             0.052083              0.052995  0.169077  0.449612   \n",
      "6475             0.041667              0.048387  0.175631  0.635659   \n",
      "6476             0.041667              0.032258  0.159823  0.542636   \n",
      "6477             0.079861              0.059908  0.133218  0.635659   \n",
      "6478             0.027778              0.046083  0.146327  0.496124   \n",
      "6479             0.079861              0.059908  0.133218  0.635659   \n",
      "6480             0.041667              0.048387  0.125506  0.658915   \n",
      "6481             0.107639              0.211982  0.167149  0.472868   \n",
      "6482             0.079861              0.064516  0.149026  0.441860   \n",
      "6483             0.072917              0.096774  0.150954  0.449612   \n",
      "6484             0.114583              0.124424  0.150569  0.480620   \n",
      "6485             0.059028              0.050691  0.148834  0.643411   \n",
      "6486             0.114583              0.221198  0.135531  0.426357   \n",
      "6487             0.097222              0.168203  0.204164  0.441860   \n",
      "6488             0.086806              0.066820  0.116252  0.465116   \n",
      "6489             0.052083              0.046083  0.133218  0.736434   \n",
      "6490             0.097222              0.078341  0.166377  0.542636   \n",
      "6491             0.093750              0.073733  0.181222  0.542636   \n",
      "6492             0.107639              0.087558  0.150183  0.565891   \n",
      "6493             0.131944              0.103687  0.154425  0.620155   \n",
      "6494             0.097222              0.078341  0.166377  0.542636   \n",
      "6495             0.107639              0.087558  0.161172  0.658915   \n",
      "6496             0.059028              0.082949  0.161558  0.519380   \n",
      "\n",
      "      sulphates   alcohol  \n",
      "0      0.129213  0.115942  \n",
      "1      0.151685  0.217391  \n",
      "2      0.123596  0.304348  \n",
      "3      0.101124  0.275362  \n",
      "4      0.101124  0.275362  \n",
      "5      0.123596  0.304348  \n",
      "6      0.140449  0.231884  \n",
      "7      0.129213  0.115942  \n",
      "8      0.151685  0.217391  \n",
      "9      0.129213  0.434783  \n",
      "10     0.191011  0.579710  \n",
      "11     0.174157  0.246377  \n",
      "12     0.230337  0.405797  \n",
      "13     0.168539  0.637681  \n",
      "14     0.252809  0.246377  \n",
      "15     0.185393  0.492754  \n",
      "16     0.078652  0.231884  \n",
      "17     0.095506  0.695652  \n",
      "18     0.174157  0.478261  \n",
      "19     0.157303  0.217391  \n",
      "20     0.095506  0.695652  \n",
      "21     0.073034  0.434783  \n",
      "22     0.146067  0.362319  \n",
      "23     0.162921  0.188406  \n",
      "24     0.140449  0.289855  \n",
      "25     0.157303  0.347826  \n",
      "26     0.123596  0.289855  \n",
      "27     0.174157  0.362319  \n",
      "28     0.151685  0.521739  \n",
      "29     0.275281  0.623188  \n",
      "...         ...       ...  \n",
      "6467   0.196629  0.507246  \n",
      "6468   0.398876  0.637681  \n",
      "6469   0.241573  0.449275  \n",
      "6470   0.162921  0.217391  \n",
      "6471   0.252809  0.652174  \n",
      "6472   0.146067  0.362319  \n",
      "6473   0.235955  0.550725  \n",
      "6474   0.314607  0.405797  \n",
      "6475   0.213483  0.565217  \n",
      "6476   0.252809  0.478261  \n",
      "6477   0.213483  0.478261  \n",
      "6478   0.213483  0.565217  \n",
      "6479   0.213483  0.478261  \n",
      "6480   0.157303  0.565217  \n",
      "6481   0.224719  0.260870  \n",
      "6482   0.325843  0.521739  \n",
      "6483   0.348315  0.507246  \n",
      "6484   0.353933  0.492754  \n",
      "6485   0.247191  0.420290  \n",
      "6486   0.314607  0.695652  \n",
      "6487   0.179775  0.173913  \n",
      "6488   0.337079  0.521739  \n",
      "6489   0.191011  0.521739  \n",
      "6490   0.297753  0.434783  \n",
      "6491   0.337079  0.217391  \n",
      "6492   0.202247  0.362319  \n",
      "6493   0.303371  0.463768  \n",
      "6494   0.297753  0.434783  \n",
      "6495   0.275281  0.318841  \n",
      "6496   0.247191  0.434783  \n",
      "\n",
      "[6497 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "scaled_df=scaler.fit_transform(data_not)\n",
    "scaled_df=(pd.DataFrame(scaled_df, columns=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']))\n",
    "print (scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Type=data['type_bin'] = np.where(data['type']=='white',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_col=data[data.columns[11:14]]\n",
    "data_tot=scaled_df.join(data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tot_w=data_tot[data_tot['type']=='white']\n",
    "data_tot_r=data_tot[data_tot['type']=='red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_r=data_tot_r[data_tot_r.columns[0:11]]\n",
    "y_r=data_tot_r['quality']\n",
    "\n",
    "X_w=data_tot_w[data_tot_w.columns[0:11]]\n",
    "y_w=data_tot_w['quality']\n",
    "\n",
    "X_r_train, X_r_test, y_r_train, y_r_test=train_test_split(X_r,y_r,test_size=0.3)\n",
    "X_w_train, X_w_test, y_w_train, y_w_test=train_test_split(X_w,y_w,test_size=0.3)\n",
    "\n",
    "svr_rbf = SVC(kernel='rbf', gamma='auto')\n",
    "svr_lin = SVC(kernel='linear',gamma='auto')\n",
    "svr_poly = SVC(kernel='poly',gamma='auto')\n",
    "svm_sigmo=svm.SVC(kernel='sigmoid', gamma='auto')\n",
    "\n",
    "y_r_rbf = svr_rbf.fit(X_r_train, y_r_train)\n",
    "R_r=y_r_rbf.predict(X_r_test)\n",
    "r_rbf=sum(R_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "y_w_rbf = svr_rbf.fit(X_w_train, y_w_train)\n",
    "R_w=y_w_rbf.predict(X_w_test)\n",
    "w_rbf=sum(R_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "y_r_lin = svr_lin.fit(X_r_train, y_r_train)\n",
    "L_r=y_r_lin.predict(X_r_test)\n",
    "r_lin=sum(L_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "y_w_lin = svr_lin.fit(X_w_train, y_w_train)\n",
    "L_w=y_w_lin.predict(X_w_test)\n",
    "w_lin=sum(L_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "y_r_poly = svr_poly.fit(X_r_train, y_r_train)\n",
    "Po_r=y_r_poly.predict(X_r_test)\n",
    "r_poly=sum(Po_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "y_w_poly = svr_poly.fit(X_w_train, y_w_train)\n",
    "Po_w=y_w_poly.predict(X_w_test)\n",
    "w_poly=sum(Po_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "y_r_sigmoid=svm_sigmo.fit(X_r_train,y_r_train)\n",
    "S_r=y_r_sigmoid.predict(X_r_test)\n",
    "r_sig=sum(S_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "y_w_sigmoid=svm_sigmo.fit(X_w_train,y_w_train)\n",
    "S_w=y_w_sigmoid.predict(X_w_test)\n",
    "w_sig=sum(S_w==y_w_test)/len(y_w_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.3\n",
    "\n",
    "Test the two SVM's using the different kernels (‘poly’, ‘rbf’, ‘sigmoid’) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf Accuracy     red : 0.6083333333333333  white: 0.5197278911564626\n",
      "poly Accuracy    red : 0.4395833333333333 white: 0.46122448979591835\n",
      "linear Accuracy  red : 0.5979166666666667  white: 0.5231292517006803\n",
      "sigmoid Accuracy red : 0.58125  white: 0.4748299319727891\n"
     ]
    }
   ],
   "source": [
    "print('rbf Accuracy    ', 'red :',r_rbf,' white:',w_rbf)\n",
    "print('poly Accuracy   ', 'red :',r_poly,'white:',w_poly)\n",
    "print('linear Accuracy ', 'red :',r_lin,' white:',w_lin)\n",
    "print('sigmoid Accuracy', 'red :',r_sig,' white:',w_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4\n",
    "Using the best SVM find the parameters that gives the best performance \n",
    "\n",
    "'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1= SVC(kernel='rbf', gamma=0.01, C=0.1)\n",
    "c2= SVC(kernel='rbf', gamma='auto', C=1)\n",
    "c3= SVC(kernel='rbf', gamma=0.001, C=10)\n",
    "c4= SVC(kernel='rbf', gamma='auto', C=100)\n",
    "c5= SVC(kernel='rbf', gamma=0.0001, C=1000)\n",
    "\n",
    "c1_r_rbf = c1.fit(X_r_train, y_r_train)\n",
    "c1_r=c1_r_rbf.predict(X_r_test)\n",
    "c1r_rbf=sum(c1_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "c1_w_rbf = c1.fit(X_w_train, y_w_train)\n",
    "c1_w=c1_w_rbf.predict(X_w_test)\n",
    "c1w_rbf=sum(c1_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "c2_r_rbf = c2.fit(X_r_train, y_r_train)\n",
    "c2_r=c2_r_rbf.predict(X_r_test)\n",
    "c2r_rbf=sum(c2_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "c2_w_rbf = c2.fit(X_w_train, y_w_train)\n",
    "c2_w=c2_w_rbf.predict(X_w_test)\n",
    "c2w_rbf=sum(c2_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "c3_r_rbf = c3.fit(X_r_train, y_r_train)\n",
    "c3_r=c3_r_rbf.predict(X_r_test)\n",
    "c3r_rbf=sum(c3_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "c3_w_rbf = c3.fit(X_w_train, y_w_train)\n",
    "c3_w=c3_w_rbf.predict(X_w_test)\n",
    "c3w_rbf=sum(c3_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "c4_r_rbf = c4.fit(X_r_train, y_r_train)\n",
    "c4_r=c4_r_rbf.predict(X_r_test)\n",
    "c4r_rbf=sum(c4_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "c4_w_rbf = c4.fit(X_w_train, y_w_train)\n",
    "c4_w=c4_w_rbf.predict(X_w_test)\n",
    "c4w_rbf=sum(c4_w==y_w_test)/len(y_w_test)\n",
    "\n",
    "c5_r_rbf = c5.fit(X_r_train, y_r_train)\n",
    "c5_r=c5_r_rbf.predict(X_r_test)\n",
    "c5r_rbf=sum(c5_r==y_r_test)/len(y_r_test)\n",
    "\n",
    "c5_w_rbf = c5.fit(X_w_train, y_w_train)\n",
    "c5_w=c5_w_rbf.predict(X_w_test)\n",
    "c5w_rbf=sum(c5_w==y_w_test)/len(y_w_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.5\n",
    "\n",
    "Compare the results with other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf Accuracy c1     red : 0.4395833333333333  white: 0.46122448979591835\n",
      "rbf Accuracy c2     red : 0.6083333333333333  white: 0.5197278911564626\n",
      "rbf Accuracy c3     red : 0.4395833333333333  white: 0.46122448979591835\n",
      "rbf Accuracy c4     red : 0.6  white: 0.5265306122448979\n",
      "rbf Accuracy c5     red : 0.60625  white: 0.5170068027210885\n"
     ]
    }
   ],
   "source": [
    "print('rbf Accuracy c1    ', 'red :',c1r_rbf,' white:',c1w_rbf)\n",
    "print('rbf Accuracy c2    ', 'red :',c2r_rbf,' white:',c2w_rbf)\n",
    "print('rbf Accuracy c3    ', 'red :',c3r_rbf,' white:',c3w_rbf)\n",
    "print('rbf Accuracy c4    ', 'red :',c4r_rbf,' white:',c4w_rbf)\n",
    "print('rbf Accuracy c5    ', 'red :',c5r_rbf,' white:',c5w_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los metodos generan resultados similares, sin embargo quien mejor representa la prediccion sigue siendo rbf, comparado a los otros metodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.6\n",
    "\n",
    "\n",
    "* Train a linear regression to predict wine quality (Continous) -- predecir continuamente la calidad de 3 a 9\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.95752544 -2.03094504 -0.33645095  2.93642304 -0.26418949  1.51137144\n",
      " -1.01089897 -2.97662758  0.58360803  1.35627374  1.79220429]\n",
      "Mean squared error: 0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X=data_tot[data_tot.columns[0:11]]\n",
    "y=data_tot['quality']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aunque el error cuadratico no es tan pequeño con los datos y las variables se puede definir y predecir la variable dependiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.7\n",
    "\n",
    "* Estimate a ridge regression with alpha equals 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.42515614 -1.78403445 -0.08730228  1.74303462 -0.5856916   1.15164668\n",
      " -0.74468497 -1.57156103  0.32195241  1.14369605  1.76032959]\n",
      "Mean squared error: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridgereg = Ridge(alpha=0.1, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print('Coefficients: \\n', ridgereg.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.01706692 -0.88067166  0.24822433  0.37314046 -0.77082965  0.37504967\n",
      " -0.25022498 -1.16026335  0.10561067  0.53283731  0.94040336]\n",
      "Mean squared error: 0.58\n"
     ]
    }
   ],
   "source": [
    "ridgereg = Ridge(alpha=1, normalize=True)\n",
    "ridgereg.fit(X_train, y_train)\n",
    "y_pred = ridgereg.predict(X_test)\n",
    "print('Coefficients: \\n', ridgereg.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.8\n",
    "\n",
    "* Estimate a lasso regression with alpha equals 0.01, 0.1 and 1.\n",
    "* Compare the coefficients with the linear regression\n",
    "* Evaluate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.         -1.33396819  0.          0.         -0.          0.\n",
      " -0.         -0.          0.          0.          1.77422724]\n",
      "Mean squared error: 0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lassoreg = Lasso(alpha=0.001, normalize=True)\n",
    "lassoreg.fit(X_train, y_train)\n",
    "print('Coefficients: \\n', lassoreg.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0. -0.  0. -0. -0.  0. -0. -0.  0.  0.  0.]\n",
      "Mean squared error: 0.58\n"
     ]
    }
   ],
   "source": [
    "lassoreg = Lasso(alpha=1, normalize=True)\n",
    "lassoreg.fit(X_train, y_train)\n",
    "print('Coefficients: \\n', lassoreg.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.9\n",
    "\n",
    "* Create a binary target\n",
    "\n",
    "* Train a logistic regression to predict wine quality (binary)\n",
    "\n",
    "* Analyze the coefficients\n",
    "\n",
    "* Evaluate the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "      <th>type_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.126667</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.308282</td>\n",
       "      <td>0.059801</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.377880</td>\n",
       "      <td>0.267785</td>\n",
       "      <td>0.217054</td>\n",
       "      <td>0.129213</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206612</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.132832</td>\n",
       "      <td>0.449612</td>\n",
       "      <td>0.151685</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.355372</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.096626</td>\n",
       "      <td>0.068106</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.121166</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.121166</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.414747</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0       0.264463          0.126667     0.216867        0.308282   0.059801   \n",
       "1       0.206612          0.146667     0.204819        0.015337   0.066445   \n",
       "2       0.355372          0.133333     0.240964        0.096626   0.068106   \n",
       "3       0.280992          0.100000     0.192771        0.121166   0.081395   \n",
       "4       0.280992          0.100000     0.192771        0.121166   0.081395   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0             0.152778              0.377880  0.267785  0.217054   0.129213   \n",
       "1             0.045139              0.290323  0.132832  0.449612   0.151685   \n",
       "2             0.100694              0.209677  0.154039  0.418605   0.123596   \n",
       "3             0.159722              0.414747  0.163678  0.364341   0.101124   \n",
       "4             0.159722              0.414747  0.163678  0.364341   0.101124   \n",
       "\n",
       "    alcohol  quality   type  type_bin  \n",
       "0  0.115942        6  white         1  \n",
       "1  0.217391        6  white         1  \n",
       "2  0.304348        6  white         1  \n",
       "3  0.275362        6  white         1  \n",
       "4  0.275362        6  white         1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2836\n",
       "5    2138\n",
       "7    1079\n",
       "4     216\n",
       "8     193\n",
       "3      30\n",
       "9       5\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tot['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tot['quality_type'] = np.where(data_tot['quality']< 6,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4113\n",
       "0    2384\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tot['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_tot[data_tot.columns[0:11]]\n",
    "y=data_tot['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='auto', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9,solver='liblinear',multi_class='auto')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[ 1.54662929 -6.6579565  -1.54319534  6.20361567 -0.79202973  4.45561891\n",
      "  -3.12472316 -3.87932198  0.90686793  3.93265128  5.82629683]]\n",
      "predicted probabilities [[0.14432673 0.85567327]\n",
      " [0.09018548 0.90981452]\n",
      " [0.47616348 0.52383652]\n",
      " ...\n",
      " [0.02057451 0.97942549]\n",
      " [0.06943923 0.93056077]\n",
      " [0.08402093 0.91597907]]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficients: \\n', logreg.coef_)\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    "print('predicted probabilities', y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate log loss 0.5107099065522029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print('calculate log loss', log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.10\n",
    "\n",
    "* Estimate a regularized logistic regression using:\n",
    "* C = 0.01, 0.1 & 1.0\n",
    "* penalty = ['l1, 'l2']\n",
    "* Compare the coefficients and the f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.48570385  0.          0.05572712  0.          0.\n",
      "   0.          0.          0.          0.11484858  0.86106117]]\n"
     ]
    }
   ],
   "source": [
    "# C=0.01 with L1 penalty\n",
    "logreg = LogisticRegression(C=0.01, penalty='l1',solver='liblinear',multi_class='auto')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5283558582075295\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09458229 -0.57751976 -0.05963997  0.30266515 -0.08513586  0.18570927\n",
      "  -0.28711263 -0.17744673  0.08110469  0.26545366  0.811105  ]]\n"
     ]
    }
   ],
   "source": [
    "# C=0.01 with L2 penalty\n",
    "logreg = LogisticRegression(C=0.01, penalty='l2',multi_class='auto',solver='liblinear')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5150941524449589\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03431397 -0.72274389 -0.10396291  0.2870452  -0.04826651  0.23034865\n",
      "  -0.34621275  0.          0.03658572  0.28394827  1.07545229]]\n"
     ]
    }
   ],
   "source": [
    "# C=0.1 with L1 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l1',solver='liblinear',multi_class='auto')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5103820803732918\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15028494 -0.71373756 -0.12352109  0.42530979 -0.05177741  0.26063747\n",
      "  -0.39039379 -0.20897562  0.10652593  0.31893787  0.98154657]]\n"
     ]
    }
   ],
   "source": [
    "# C=0.1 with L2 penalty\n",
    "logreg = LogisticRegression(C=0.1, penalty='l2',multi_class='auto',solver='liblinear')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5107464017663997\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14534226 -0.73542897 -0.13151383  0.42673088 -0.04627053  0.26940738\n",
      "  -0.40153886 -0.18799125  0.10213793  0.32203355  1.01751604]]\n"
     ]
    }
   ],
   "source": [
    "# C=1.0 with L1 penalty\n",
    "logreg = LogisticRegression(C=1.0, penalty='l1',solver='liblinear',multi_class='auto')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5105970959632321\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16275776 -0.73267209 -0.13346015  0.44864345 -0.04590714  0.27247935\n",
      "  -0.40699637 -0.22212463  0.11268129  0.32702151  1.00216482]]\n"
     ]
    }
   ],
   "source": [
    "# C=1.0 with L2 penalty\n",
    "logreg = LogisticRegression(C=1.0, penalty='l2',multi_class='auto',solver='liblinear')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5107066258331013\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test_scaled)\n",
    "print(log_loss(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
